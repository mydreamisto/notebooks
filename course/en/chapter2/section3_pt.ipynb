{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPU5GyZdNiFz"
      },
      "source": [
        "# Models (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyFhd-CZNiF0"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOGZsmJYNiF0"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a model from the default configuration initializes it with random values:\n",
        "\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "config = BertConfig()\n",
        "model = BertModel(config)\n",
        "## Model is randomly initialized!\n",
        "print(config)\n",
        "\n",
        "#The model just be created is not a pretrained model\n",
        "#it will output gibberish; it needs to be trained first.\n",
        "\n",
        "#We could train the model from scratch on the task at hand, but as just we learned before\n",
        "#this would require a long time and a lot of data, and it would have a non-negligible environmental impact.\n",
        "#To avoid unnecessary and duplicated effort,\n",
        "#it’s imperative to be able to share and reuse models that have already been trained."
      ],
      "metadata": {
        "id": "ALT0a1kEOkfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prl-NfLzNiF2"
      },
      "outputs": [],
      "source": [
        "#Loading a Transformer model that is already trained is simple — we can do this using the from_pretrained() method:\n",
        "from transformers import BertModel\n",
        "\n",
        "model = BertModel.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7FYaB-0NiF2"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"directory_on_my_computer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I6bxR8SNiF2"
      },
      "outputs": [],
      "source": [
        "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dUXD06DNiF3"
      },
      "outputs": [],
      "source": [
        "#The tokenizer converts these to vocabulary indices which are typically called input IDs\n",
        "encoded_sequences = [\n",
        "    [101, 7592, 999, 102],\n",
        "    [101, 4658, 1012, 102],\n",
        "    [101, 3835, 999, 102],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEtWW1d5NiF3"
      },
      "outputs": [],
      "source": [
        "#converting it to a tensor\n",
        "import torch\n",
        "\n",
        "model_inputs = torch.tensor(encoded_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9e6lqCSNiF3"
      },
      "outputs": [],
      "source": [
        "output = model(model_inputs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Models (PyTorch)",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}