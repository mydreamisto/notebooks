{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9-F_CnPf22Z"
      },
      "source": [
        "# Handling multiple sequences (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw80V6CKf22a"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4QX-k6Yf22a"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA91zCAOf22b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = torch.tensor(ids)\n",
        "# This line will fail.\n",
        "model(input_ids)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here we tried to do everything the tokenizer did behind the scenes when we applied it to one sequence.\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "# 第一段代码的操作\n",
        "#torch.tensor(ids)只是一个一维tensor，其shape类似于 [n]，其中 n 是token的数量\n",
        "#torch.tensor(ids)生成的tensor shape是 torch.Size([14])\n",
        "# Transformers 中的模型通常期望输入是一个批次（batch）的数据，即使你只有一个句子，也需要将其作为一个批次来处理。这个额外的维度表示批次大小，即使这个批次中只有一个元素。\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "#这里的 tokenizer.tokenize(sequence) 期望 sequence 是一个字符串，而不是一个字符串列表。因此，当你传递一个列表时，会导致错误。\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = torch.tensor(ids)\n",
        "print(\"First method:\", input_ids)\n",
        "print(\"Shape of first method:\", input_ids.shape)\n",
        "\n",
        "# 第二段代码的操作\n",
        "#tokenizer(sequence, return_tensors=\"pt\")不仅仅将标记列表转换为张量，还将这个张量包装在一个额外的维度中。最终得到的张量的形状类似于 [1, n]，这里的 1 就是添加的维度。\n",
        "#tokenizer(sequence, return_tensors=\"pt\")生成的张量形状是 torch.Size([1, 14])\n",
        "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
        "print(\"Second method:\", tokenized_inputs[\"input_ids\"])\n",
        "print(\"Shape of second method:\", tokenized_inputs[\"input_ids\"].shape)\n",
        "\n",
        "print(\"------------------------------------------------------------------------------------------------\")\n",
        "input_ids = torch.tensor([ids])\n",
        "print(\"Input IDs:\", input_ids)\n",
        "output = model(input_ids)\n",
        "print(\"Logits:\", output.logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bsvF_Bt5Lfw",
        "outputId": "4b96edc3-122e-4523-b411-84cef74977e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First method: tensor([ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "         2026,  2878,  2166,  1012])\n",
            "Shape of first method: torch.Size([14])\n",
            "Second method: tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102]])\n",
            "Shape of second method: torch.Size([1, 16])\n",
            "------------------------------------------------------------------------------------------------\n",
            "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
            "          2026,  2878,  2166,  1012]])\n",
            "Logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "#if there is only one single sequence, create a batch with a single sequence, that means a batch of two identical sequences\n",
        "batched_ids = [ids,ids]\n",
        "\"\"\"\n",
        "在深度学习中，模型通常是批量处理输入数据的，而不是单个样本。将单个序列转换为批次的目的是模拟多个样本的输入。\n",
        "这里通过 [ids,ids] 这种方式创建了一个包含两个相同序列的批次。\n",
        "使用 [] 是因为在 Python 中，列表可以用来存储多个元素，这里将两个相同的序列 ID 列表存储在一起，形成一个批次。\n",
        "\"\"\"\n",
        "input_ids = torch.tensor(batched_ids)\n",
        "output = model(input_ids)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "-06u7okF_AiO",
        "outputId": "f3978c4b-b93c-4ff8-85c2-66b393f2ba96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SequenceClassifierOutput(loss=None, logits=tensor([[-2.7276,  2.8789],\n",
            "        [-2.7276,  2.8789]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yOysyU1f22c"
      },
      "outputs": [],
      "source": [
        "# this  batched_ids cannot be converted to a tensor directly\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6u8EOZS7f22c"
      },
      "outputs": [],
      "source": [
        "# the padding principle: padding token\n",
        "padding_id = 100\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, padding_id],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSkPxkaNf22c",
        "outputId": "46eb0cd8-01c8-469f-f8ad-049571ea8a3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)\n",
              "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)\n",
              "tensor([[ 1.5694, -1.3895],\n",
              "        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "print(model(torch.tensor(sequence1_ids)).logits)\n",
        "print(model(torch.tensor(sequence2_ids)).logits)\n",
        "print(model(torch.tensor(batched_ids)).logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yodgTVpTf22d",
        "outputId": "74cbbfbb-88ce-4ed8-e2eb-d248cfc79e45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.5694, -1.3895],\n",
              "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the result above in forth row is wrong,\n",
        "# bacause the attention layers contextualizes each token, including the padding token\n",
        "# so we need to tell the attention layers to ignore the padding tokens by an attention mask:\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, tokenizer.pad_token_id],\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1],\n",
        "    [1, 1, 0],\n",
        "]\n",
        "\n",
        "# the attention mask need to be converted to tensor, too.\n",
        "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8XIRHmzf22d"
      },
      "outputs": [],
      "source": [
        "sequence = sequence[:max_sequence_length]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Handling multiple sequences (PyTorch)",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}