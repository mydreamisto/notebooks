{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNuKbL/sklabX6/9Ex2Ph+l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mydreamisto/notebooks/blob/main/course/en/chapter3/Important_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try it out! Fine-tune a model on the GLUE SST-2 dataset, using the data processing you did in section 2."
      ],
      "metadata": {
        "id": "O-9U6f1G3Di8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "BMgL8YVH4ADI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "raw_dataset = load_dataset(\"gimmaru/glue-sst2\")\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer, DataCollatorWithPadding\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "model = AutoModel.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
        "\n",
        "def tokenize_function(example):\n",
        "  return tokenizer(example[\"sentence\"], truncation = True)\n",
        "tokenized_dataset = raw_dataset.map(tokenize_function, batched = True)\n",
        "samples = tokenized_dataset[\"validation\"][:8]\n",
        "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence\"]}\n",
        "batch = data_collator(samples)"
      ],
      "metadata": {
        "id": "LKM5WZhT4C33"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_dataset = raw_validation_dataset.map(tokenize_function, batched = True)\n",
        "# # 把原始文本以及无用index剔除\n",
        "# samples = {k: v for k, v in tokenized_dataset.items() if k not in [\"idx\", \"sentence\"]}\n",
        "# batch = data_collator(samples)"
      ],
      "metadata": {
        "id": "WrwhFf9r52t4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_dataset = raw_validation_dataset.map(tokenize_function, batched = True)\n",
        "# samples = {k: v for k, v in tokenized_dataset.items() if k not in [\"idx\", \"sentence\"]}\n",
        "# batch = data_collator(samples)"
      ],
      "metadata": {
        "id": "ysViOiXU8lgW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# it is of DatasetDic(dictionary) tpye, which has items() method\n",
        "raw_dataset = load_dataset(\"gimmaru/glue-sst2\")\n",
        "# it is of Dataset(set) tpye, which has no items() method\n",
        "raw_validation_dataset = raw_dataset[\"validation\"]\n",
        "\n",
        "from transformers import AutoModel, AutoTokenizer, DataCollatorWithPadding\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "model = AutoModel.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
        "\n",
        "# 定义的tokenize_function函数其参数是一个DatasetDic(dictionary)，其返回值也是一个DatasetDic(dictionary).\n",
        "def tokenize_function(example):\n",
        "  # example[\"sentence\"] is of Dataset type\n",
        "  return tokenizer(example[\"sentence\"], truncation = True)\n",
        "# Dataset.map() 对Dataset里的每个元素都调用一次tokenize_function函数，其返回的也是字典\n",
        "tokenized_dataset = raw_dataset.map(tokenize_function, batched = True)\n",
        "\n",
        "# it is of dictionary tpye, which has items() method\n",
        "samples = tokenized_dataset[\"validation\"][:8]\n",
        "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence\"]}\n",
        "batch = data_collator(samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLyGcMES9q8d",
        "outputId": "be9a4555-7492-4509-886b-ca663b924ad0"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(raw_dataset[\"validation\"]))\n",
        "raw_dataset[\"validation\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqYR--zEGaxG",
        "outputId": "3d9fc4a2-3faa-468d-d40a-f1daded80005"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'datasets.arrow_dataset.Dataset'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['sentence', 'label', 'idx'],\n",
              "    num_rows: 872\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(raw_dataset[\"validation\"][\"sentence\"]))\n",
        "raw_dataset[\"validation\"][\"sentence\"]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vOJJthZ0F7td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(raw_dataset[\"validation\"][\"sentence\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbrYFKvpGztd",
        "outputId": "0cc7a841-e003-4125-ba4c-488239404836"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "872"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ePfH37zCxj_",
        "outputId": "82bb380f-eb9e-4c76-cdff-8517ca1db7c8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    validation: Dataset({\n",
              "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
              "        num_rows: 872\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(raw_dataset)) # it has items() method\n",
        "raw_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qn5wcR_-EyI",
        "outputId": "bd70012c-9276-4b20-cbb1-e7c470945ca7"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'datasets.dataset_dict.DatasetDict'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    validation: Dataset({\n",
              "        features: ['sentence', 'label', 'idx'],\n",
              "        num_rows: 872\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_validation_dataset = raw_dataset[\"validation\"]\n",
        "print(type(raw_validation_dataset)) # it has no items() method\n",
        "raw_validation_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABamAmoG-KQ_",
        "outputId": "869555d9-9fcf-4e46-8473-fa302a4b0170"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'datasets.arrow_dataset.Dataset'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['sentence', 'label', 'idx'],\n",
              "    num_rows: 872\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ]
}