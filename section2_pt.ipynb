{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mydreamisto/notebooks/blob/main/section2_pt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i06llBIRQlzT"
      },
      "source": [
        "# Processing the data (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfN6PHdaQlzU"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "aH8rbGYBRRCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onnm32GZQlzU"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oepr0w3YQlzV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Same as before\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequences = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"This course is amazing!\",\n",
        "]\n",
        "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# This is new\n",
        "batch[\"labels\"] = torch.tensor([1, 1])\n",
        "\n",
        "optimizer = AdamW(model.parameters())\n",
        "loss = model(**batch).loss\n",
        "loss.backward()\n",
        "optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rtc-flR-QlzV"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9Jj9cKcQlzW"
      },
      "outputs": [],
      "source": [
        "raw_train_dataset = raw_datasets[\"train\"]\n",
        "raw_train_dataset[0][\"sentence1\"] # 一层一层往下走，所以一层一层地根据包含关系输入"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bZOtS0jQlzW"
      },
      "outputs": [],
      "source": [
        "raw_train_dataset.features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
        "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])\n",
        "print(tokenized_sentences_1)\n",
        "print(tokenized_sentences_2)"
      ],
      "metadata": {
        "id": "uV0k3lRXUAKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the 15th row of the train dataset\n",
        "tokenized_sentences_15_1 = tokenizer(raw_datasets[\"train\"][14][\"sentence1\"])\n",
        "tokenized_sentences_15_2 = tokenizer(raw_datasets[\"train\"][14][\"sentence2\"])\n",
        "print(tokenized_sentences_15_1)\n",
        "print(tokenized_sentences_15_2)\n",
        "print(\"----------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "tokenized_sentences_15_both = tokenizer(raw_datasets[\"train\"][14][\"sentence1\"], raw_datasets[\"train\"][14][\"sentence1\"])\n",
        "print(tokenized_sentences_15_both)"
      ],
      "metadata": {
        "id": "Pq3bdbPYVO66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decode the 15th element\n",
        "print(tokenizer.convert_ids_to_tokens(tokenized_sentences_15_both[\"input_ids\"]))"
      ],
      "metadata": {
        "id": "p-hg4NWChlzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-KHak67QlzX"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
        "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eHRB2xcQlzX"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = tokenizer(\n",
        "    raw_datasets[\"train\"][\"sentence1\"],\n",
        "    raw_datasets[\"train\"][\"sentence2\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenizer(\n",
        "    raw_datasets[\"train\"][\"sentence1\"],\n",
        "    raw_datasets[\"train\"][\"sentence2\"],\n",
        "    padding = True,\n",
        "    truncation = True\n",
        ")"
      ],
      "metadata": {
        "id": "tUtu-1JYZpL1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 以下需要重点理解："
      ],
      "metadata": {
        "id": "IlVtxUDVigSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "  return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation = True)\n",
        "# Dataset.map() 方法用于对数据集中的每个元素应用一个函数：\n",
        "# 应用 tokenize_function 到所有数据集\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched = True)\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "BF-9aqHXnH5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Padding"
      ],
      "metadata": {
        "id": "_pj6q6yDoNSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DataCollatorWithPadding类：用于数据填充**\n",
        "\n",
        "DataCollatorWithPadding函数（tokenizer属性：指明填充标记和填充位置）是一个collate *function（将多个样本组合成一个batch批次的函数），能够将样本转换为* PyTorch tensors并将它们连接起来"
      ],
      "metadata": {
        "id": "poh3dC9mmDH2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sj8a5LtaQlzX"
      },
      "outputs": [],
      "source": [
        "# 在实际操作中，当我们处理数据集并将其分批（batch）时，需要确保每个批次中的元素长度相同，以便能够将它们输入到深度学习模型中进行处理。\n",
        "# 因为不同的文本输入长度可能不同，为了将它们组合成一个批次，需要进行填充（padding）操作，使它们的长度一致。\n",
        "\n",
        "# DataCollatorWithPadding类。这个类是用于数据填充。\n",
        "# tokenizer：这是一个已经实例化的分词器对象。将其传递给 DataCollatorWithPadding 的目的是让它知道如何进行填充操作。具体来说，分词器可以告诉 DataCollatorWithPadding 以下信息：\n",
        "# ①使用哪个填充标记（padding token）：不同的分词器可能使用不同的填充标记，比如对于一些自然语言处理任务，可能使用 [PAD] 作为填充标记。\n",
        "# ②填充的位置：有些模型期望填充在输入的左边，有些则期望在右边，分词器可以提供这方面的信息。\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLrTyuoUQlzX"
      },
      "outputs": [],
      "source": [
        "samples = tokenized_datasets[\"train\"][:8]\n",
        "# 这是一个字典推导式。它遍历 samples 中的每一个键值对（使用 items() 方法），并对键 k 和值 v 进行筛选。\n",
        "# if k not in [\"idx\", \"sentence1\", \"sentence2\"] 是一个条件判断，只有当键 k 不在列表 [\"idx\", \"sentence1\", \"sentence2\"] 中时，才将该键值对包含在新的字典 samples 中。\n",
        "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
        "[len(x) for x in samples[\"input_ids\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_o5PH_8XQlzX"
      },
      "outputs": [],
      "source": [
        "batch = data_collator(samples)\n",
        "{k: v.shape for k, v in batch.items()}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}